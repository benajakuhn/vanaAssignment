{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Gradient Descent - VANA\n",
    "## Task\n",
    "The aim of this task is to get a basic understanding of numerical approximation methods in higher approximation methods in higher dimensions, in particular for the gradient descent and its practical application. For this purpose, we created a Jupyter notebook and loaded and explored the MNIST dataset. We have then created and trained a neural network to classify the images correctly.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Importing MNIST Dataset\n",
    "\n",
    "In the first section, the MNIST (Modified National Institute of Standards and Technology) data is imported into the project. This data set consists of a collection of handwritten digits which is the basis for training a machine learning system. The import is performed by the package torchvision. Package description: \"The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision”."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import torchvision.datasets as datasets\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "len(mnist_trainset)\n",
    "\n",
    "train_image_zero, train_target_zero = mnist_trainset[0];"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Visualizing the MNIST Dataset\n",
    "\n",
    "In the following section, we visualize the **MNIST dataset** and examine its properties in detail. This includes graphical representations of character distributions in both the training and test sets, as well as samples of the first characters of each type.\n",
    "\n",
    "#### **Dataset Overview**\n",
    "\n",
    "- **Total Images:** 70,000\n",
    "  - **Training Images:** 60,000\n",
    "  - **Test Images:** 10,000\n",
    "- **Image Specifications:**\n",
    "  - **Size:** 28x28 pixels\n",
    "  - **Color:** Black and white\n",
    "\n",
    "#### **Key Observations**\n",
    "\n",
    "- **Data Distribution:**\n",
    "  - Histograms reveal that the data is consistently and uniformly distributed across both training and test sets, with no significant outliers.\n",
    "  \n",
    "- **Label Distribution:**\n",
    "  - The distribution of labels in the MNIST test dataset closely mirrors that of the training dataset, with only minor deviations in percentage distribution.\n",
    "  \n",
    "- **Labeling:**\n",
    "  - Each handwritten digit image is paired with a corresponding label that accurately identifies the digit it represents.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def visualize_datasets(train_dataset, test_dataset):\n",
    "    def get_counts_samples(dataset):\n",
    "        counts = Counter()\n",
    "        samples = {}\n",
    "        for image, label in dataset:\n",
    "            counts[label] += 1\n",
    "            if label not in samples:\n",
    "                samples[label] = image\n",
    "        return counts, samples\n",
    "\n",
    "    train_counts, train_samples = get_counts_samples(train_dataset)\n",
    "    test_counts, test_samples = get_counts_samples(test_dataset)\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10), constrained_layout=True)\n",
    "    gs = gridspec.GridSpec(2,2, figure=fig)\n",
    "\n",
    "    gs_images_train = gridspec.GridSpecFromSubplotSpec(2,5, subplot_spec=gs[0,0], wspace=0.1, hspace=0.1)\n",
    "    for i in range(10):\n",
    "        ax = fig.add_subplot(gs_images_train[i])\n",
    "        ax.imshow(train_samples[i], cmap='gray')\n",
    "        ax.set_title(f'Label: {i}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    ax1 = fig.add_subplot(gs[0,1])\n",
    "    labels_train, counts_train = zip(*sorted(train_counts.items()))\n",
    "    ax1.bar(labels_train, counts_train, color='lightblue')\n",
    "    ax1.set_xlabel('Labels')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.set_title('Label Distribution Train Set')\n",
    "    ax1.set_xticks(labels_train)\n",
    "\n",
    "    gs_images_test = gridspec.GridSpecFromSubplotSpec(2,5, subplot_spec=gs[1,0], wspace=0.1, hspace=0.1)\n",
    "    for i in range(10):\n",
    "        ax = fig.add_subplot(gs_images_test[i])\n",
    "        ax.imshow(test_samples[i], cmap='gray')\n",
    "        ax.set_title(f'Label: {i}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    ax3 = fig.add_subplot(gs[1,1])\n",
    "    labels_test, counts_test = zip(*sorted(test_counts.items()))\n",
    "    ax3.bar(labels_test, counts_test, color='lightgreen')\n",
    "    ax3.set_xlabel('Labels')\n",
    "    ax3.set_ylabel('Count')\n",
    "    ax3.set_title('Label Distribution Test Set')\n",
    "    ax3.set_xticks(labels_test)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(train_counts)\n",
    "    print(train_counts.most_common())\n",
    "    print(test_counts)\n",
    "    print(test_counts.most_common())\n",
    "\n",
    "visualize_datasets(mnist_trainset, mnist_testset);\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Mathematical Concepts in Neural Networks$\n",
    "In this part the mathematical concepts in neural networks are explained. These form the basis of our neural network implementation and training process. The following concepts were used by us:\n",
    "\n",
    "### 1. Training Loop\n",
    "*Explanation:* A training loop is a sequence of steps where the model learns: data goes through the model, the error is calculated, and parameters are adjusted.  \n",
    "*Example:* An image of a \"5\" goes into the network. If it mistakenly predicts \"3,\" the loop calculates the error and adjusts the weights so that it better recognizes \"5\" next time.\n",
    "\n",
    "*Mathematics:*\n",
    "- *Forward Pass:* $\\hat{y} = f(X; \\theta)$\n",
    "- *Loss Calculation:* $L = \\text{Loss}(\\hat{y}, y)$\n",
    "- *Backward Pass:* $\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial \\theta}$\n",
    "- *Update Parameters:* $\\theta_{\\text{new}} = \\theta - \\alpha \\cdot \\frac{\\partial L}{\\partial \\theta}$\n",
    "\n",
    "### 2. Neural Networks\n",
    "*Explanation:* A neural network processes the image layer by layer, identifying features and eventually predicting a number.  \n",
    "*Example:* The image of an \"8\" goes through the first layer, which detects certain features (like edges and lines), then through the next layer, which identifies more complex patterns, until the network outputs a high probability for \"8.\"\n",
    "\n",
    "*Mathematics:*\n",
    "- *Layer 1:* $z_1 = W_1 \\cdot x + b_1$\n",
    "- *Layer 2:* $z_2 = W_2 \\cdot z_1 + b_2$\n",
    "- *Layer 3 (output):* $z_3 = W_3 \\cdot z_2 + b_3$\n",
    "- *Activation per Layer (using ReLU as an example):* $a_i = \\text{ReLU}(z_i)$\n",
    "\n",
    "### 3. Linear Layer\n",
    "*Explanation:* A linear layer takes the pixel values of images and calculates a prediction value for each digit.  \n",
    "*Example:* If an image of a \"5\" goes into the layer, it multiplies pixel values by weights and adds a \"bias\" (a constant) to estimate whether the number is \"5\" or another digit.\n",
    "\n",
    "*Mathematics:*\n",
    "- *Forward Propagation:* $Z = X \\cdot W + B$\n",
    "  - $X$ is the input matrix (shape: $N \\times l$),\n",
    "  - $W$ are weights (shape: $l \\times O$),\n",
    "  - $B$ is the bias vector (shape: $O$).\n",
    "\n",
    "### 4. Activation Functions\n",
    "*Explanation:* Activation functions like ReLU (Rectified Linear Unit) help the network learn complex patterns by creating non-linear relationships.  \n",
    "*Example:* After a ReLU filter, an image of a \"3\" might only show positive pixel values, helping the model ignore the background and focus on the actual digit.\n",
    "\n",
    "*Mathematics:*\n",
    "- *ReLU (Rectified Linear Unit):* $\\text{ReLU}(x) = \\max(0, x)$\n",
    "- *Sigmoid:* $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- *Softmax (used for probability distribution over classes):* $\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$\n",
    "\n",
    "### 5. One-Hot Encoding\n",
    "*Explanation:* One-hot encoding represents each digit as a vector with only one \"1\" in it.  \n",
    "*Example:* If the image shows a \"7,\" it is encoded as [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]. For a \"3,\" it would be [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]. This allows the model to identify the digit clearly.\n",
    "\n",
    "### 6. Loss Functions\n",
    "*Explanation:* The loss function measures how far the network’s predictions are from the actual values.  \n",
    "*Example:* If the model misclassifies an image of a \"9\" as a \"4,\" the loss function shows a large error. For the MNIST dataset, categorical cross-entropy is commonly used, minimizing the error as much as possible.\n",
    "\n",
    "*Mathematics:*\n",
    "- *Mean Squared Error (MSE):* $\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}(x_i))^2$\n",
    "- *Binary Cross-Entropy (BCE):* $\\text{BCE}(y, \\hat{y}) = - \\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$\n",
    "- *Categorical Cross-Entropy (CE):* $\\text{CE}(y, \\hat{y}) = - \\sum_{i=1}^N y_i \\log(\\hat{y}_i)$\n",
    "\n",
    "### 7. Hyperparameter Tuning\n",
    "*Explanation:* This means trying different settings (like learning rate) to improve the model.  \n",
    "*Example:* If the model learns too slowly, we can increase the learning rate. We test different rates and layer sizes until the model achieves the highest accuracy on test images.\n",
    "\n",
    "*Mathematics:* Grid search or random search is commonly used to find optimal hyperparameters such as:\n",
    "- *Learning Rate ($\\alpha$)* – the step size during gradient descent.\n",
    "- *Number of Hidden Layers* – additional layers in the network for better feature extraction.\n",
    "  \n",
    "### 8. Evaluation\n",
    "*Explanation:* After training, evaluation checks how often the network correctly recognized digits.  \n",
    "*Example:* \"Accuracy\" shows how many images were classified correctly. If the model reaches 95% accuracy, it means it recognized the correct digit in 95 out of 100 images.\n",
    "\n",
    "*Mathematics:*\n",
    "- *Accuracy:* $\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}$\n",
    "- *Precision:* $\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$\n",
    "- *Recall:* $\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$\n",
    "- *F1 Score:* $\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "\n",
    "### 9. Evaluating Training\n",
    "*Explanation:* After each round of training (epoch), we compare the error for training and test images.  \n",
    "*Example:* If the error decreases for training images but increases for test images, the model has \"overfitted,\" meaning it memorized the training data but struggles with new images.\n",
    "\n",
    "### 10. Histograms\n",
    "*Explanation:* A histogram shows how often each digit appears in the dataset.  \n",
    "*Example:* In the MNIST dataset, we might count how often each digit (0-9) appears and create a histogram. There may be 5000 \"0s,\" 5000 \"1s,\" and so on. This helps understand the distribution of digits.\n",
    "\n",
    "*Mathematics:*  \n",
    "Given digits $d$ from 0 to 9, count frequency per digit and plot:\n",
    "- Frequency $= \\text{count}(d_i)$ for each $i$ in [0, 9]."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Linear Layer Explained\n",
    "\n",
    "A **Linear Layer**, also known as a **Fully Connected Layer**, is a fundamental building block in neural networks. Its main purpose is to transform input data into a different space, making it easier for the network to learn and make accurate predictions.\n",
    "\n",
    "#### Structure\n",
    "\n",
    "- **Weights $ \\mathbf{W} $**: These are the core parameters of the layer. Each weight connects an input feature to an output neuron. The weights are initialized with small random values to start the learning process.\n",
    "  \n",
    "- **Biases $ \\mathbf{B} $**: These are additional parameters that allow the layer to adjust the output independently of the input. Biases help the model fit the data better by providing flexibility.\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "1. **Forward Pass**:\n",
    "   - **Input $\\mathbf{X}$**: The data you feed into the layer.\n",
    "   - **Computation**: The layer multiplies the input by the weights and then adds the biases.\n",
    "     $$\n",
    "     \\mathbf{Z} = \\mathbf{X} \\cdot \\mathbf{W} + \\mathbf{B}\n",
    "     $$\n",
    "   - **Output $\\mathbf{Z}$**: The transformed data that moves to the next layer in the network.\n",
    "\n",
    "2. **Backward Pass**:\n",
    "   - **Gradient Calculation**: During training, the network adjusts the weights and biases to minimize the error in its predictions. It calculates how much each weight and bias contributed to the error.\n",
    "   - **Updating Parameters**: Using these gradients, the weights and biases are updated in the opposite direction of the gradient to reduce the error.\n",
    "      $$\n",
    "     \\mathbf{W} = \\mathbf{W} - \\text{learning rate} \\times \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}}\n",
    "     $$\n",
    "     $$\n",
    "     \\mathbf{B} = \\mathbf{B} - \\text{learning rate} \\times \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{B}}\n",
    "     $$\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, input_dim, output_dim, learning_rate=0.01):\n",
    "        # He initialization for weights\n",
    "        self.W = np.random.randn(input_dim, output_dim) * np.sqrt(2. / input_dim)\n",
    "        self.B = np.zeros(output_dim)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Compute the output: Z = X * W + B\n",
    "        self.X = X\n",
    "        self.Z = X @ self.W + self.B\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, dL_dZ):\n",
    "        # Compute derivatives for the backward pass\n",
    "        self.dL_dW = self.X.T @ dL_dZ\n",
    "        self.dL_dB = np.sum(dL_dZ, axis=0)\n",
    "        self.dL_dX = dL_dZ @ self.W.T\n",
    "        return self.dL_dX\n",
    "\n",
    "    def update_parameters(self):\n",
    "        # Parameter update: θ_new = θ − α * ∂L/∂θ\n",
    "        self.W -= self.learning_rate * self.dL_dW\n",
    "        self.B -= self.learning_rate * self.dL_dB"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Unit Test Calculations for `test_linear_layer`\n",
    "The following shows the calculations for the forward pass, backward pass, and parameter update of the `LinearLayer` class. These calculations were made by hand and are used to verify the correctness of the implementation.\n",
    "\n",
    "#### Forward Pass Calculation:\n",
    "\n",
    "$$\n",
    "\\mathbf{Z} = \\mathbf{X} \\cdot \\mathbf{W} + \\mathbf{B}\n",
    "$$\n",
    "\n",
    "Substituting the values:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_{\\text{test}} = \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix}, \\quad \n",
    "\\mathbf{W}_{\\text{test}} = \\begin{bmatrix} 0.5 & -0.5 \\\\ 1.0 & -1.0 \\end{bmatrix}, \\quad \n",
    "\\mathbf{B}_{\\text{test}} = \\begin{bmatrix} 0.1 & -0.2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Matrix Multiplication:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_{\\text{test}} \\cdot \\mathbf{W}_{\\text{test}} = \n",
    "\\begin{bmatrix}\n",
    "1.0 \\times 0.5 + 2.0 \\times 1.0 & 1.0 \\times (-0.5) + 2.0 \\times (-1.0) \\\\\n",
    "3.0 \\times 0.5 + 4.0 \\times 1.0 & 3.0 \\times (-0.5) + 4.0 \\times (-1.0)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2.5 & -2.5 \\\\\n",
    "5.5 & -5.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Addition of Bias:\n",
    "\n",
    "$$\n",
    "\\mathbf{Z}_{\\text{expected}} = \n",
    "\\begin{bmatrix}\n",
    "2.5 & -2.5 \\\\\n",
    "5.5 & -5.5\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.1 & -0.2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2.6 & -2.7 \\\\\n",
    "5.6 & -5.7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Backward Pass Calculation:\n",
    "\n",
    "##### a) Gradient with respect to Inputs $ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}} $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}} \\cdot \\mathbf{W}^T\n",
    "$$\n",
    "\n",
    "Substituting the values:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}}_{\\text{test}} = \\begin{bmatrix} 0.5 & -0.5 \\\\ 0.5 & -0.5 \\end{bmatrix}, \\quad \n",
    "\\mathbf{W}_{\\text{test}}^T = \\begin{bmatrix} 0.5 & 1.0 \\\\ -0.5 & -1.0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Calculation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}} =\n",
    "\\begin{bmatrix}\n",
    "0.5 \\times 0.5 + (-0.5) \\times (-0.5) & 0.5 \\times 1.0 + (-0.5) \\times (-1.0) \\\\\n",
    "0.5 \\times 0.5 + (-0.5) \\times (-0.5) & 0.5 \\times 1.0 + (-0.5) \\times (-1.0)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.5 & 1.0 \\\\\n",
    "0.5 & 1.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### b) Gradient with respect to Weights $ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}} $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}} = \\mathbf{X}^T \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}}\n",
    "$$\n",
    "\n",
    "Substituting the values:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_{\\text{test}}^T = \\begin{bmatrix} 1.0 & 3.0 \\\\ 2.0 & 4.0 \\end{bmatrix}, \\quad \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}}_{\\text{test}} = \\begin{bmatrix} 0.5 & -0.5 \\\\ 0.5 & -0.5 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Calculation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}} =\n",
    "\\begin{bmatrix}\n",
    "1.0 \\times 0.5 + 3.0 \\times 0.5 & 1.0 \\times (-0.5) + 3.0 \\times (-0.5) \\\\\n",
    "2.0 \\times 0.5 + 4.0 \\times 0.5 & 2.0 \\times (-0.5) + 4.0 \\times (-0.5)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2.0 & -2.0 \\\\\n",
    "3.0 & -3.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### c) Gradient with respect to Bias $ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{B}} $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{B}} = \\sum_{i=1}^{m} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}_i} = \\begin{bmatrix} 1.0 & -1.0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Parameter Update Calculation:\n",
    "\n",
    "##### a) Updating Weights $ \\mathbf{W} $:\n",
    "\n",
    "$$\n",
    "\\mathbf{W}_{\\text{updated\\_expected}} = \\mathbf{W}_{\\text{test}} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}}\n",
    "$$\n",
    "\n",
    "Substituting the values:\n",
    "\n",
    "$$\n",
    "\\eta = 0.01, \\quad \n",
    "\\mathbf{W}_{\\text{test}} = \\begin{bmatrix} 0.5 & -0.5 \\\\ 1.0 & -1.0 \\end{bmatrix}, \\quad \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}} = \\begin{bmatrix} 2.0 & -2.0 \\\\ 3.0 & -3.0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Calculation:\n",
    "\n",
    "$$\n",
    "\\mathbf{W}_{\\text{updated\\_expected}} = \n",
    "\\begin{bmatrix}\n",
    "0.5 & -0.5 \\\\\n",
    "1.0 & -1.0\n",
    "\\end{bmatrix}\n",
    "-\n",
    "0.01 \\times \n",
    "\\begin{bmatrix}\n",
    "2.0 & -2.0 \\\\\n",
    "3.0 & -3.0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.5 - 0.02 & -0.5 + 0.02 \\\\\n",
    "1.0 - 0.03 & -1.0 + 0.03\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.48 & -0.48 \\\\\n",
    "0.97 & -0.97\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### b) Updating Bias $ \\mathbf{B} $:\n",
    "\n",
    "$$\n",
    "\\mathbf{B}_{\\text{updated\\_expected}} = \\mathbf{B}_{\\text{test}} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{B}}\n",
    "$$\n",
    "\n",
    "Substituting the values:\n",
    "\n",
    "$$\n",
    "\\eta = 0.01, \\quad \n",
    "\\mathbf{B}_{\\text{test}} = \\begin{bmatrix} 0.1 & -0.2 \\end{bmatrix}, \\quad \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{B}} = \\begin{bmatrix} 1.0 & -1.0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Calculation:\n",
    "\n",
    "$$\n",
    "\\mathbf{B}_{\\text{updated\\_expected}} = \n",
    "\\begin{bmatrix}\n",
    "0.1 & -0.2\n",
    "\\end{bmatrix}\n",
    "-\n",
    "0.01 \\times \n",
    "\\begin{bmatrix}\n",
    "1.0 & -1.0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.1 - 0.01 & -0.2 + 0.01\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.09 & -0.19\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Unit tests\n",
    "def test_linear_layer():\n",
    "    # Fixed values for inputs, weights, and learning rate\n",
    "    X_test = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "    W_test = np.array([[0.5, -0.5], [1.0, -1.0]])\n",
    "    B_test = np.array([0.1, -0.2])\n",
    "    learning_rate = 0.01\n",
    "    dL_dZ_test = np.array([[0.5, -0.5], [0.5, -0.5]])\n",
    "\n",
    "    # Instantiate the layer and set fixed parameters\n",
    "    layer = LinearLayer(input_dim=2, output_dim=2, learning_rate=learning_rate)\n",
    "    layer.W = W_test\n",
    "    layer.B = B_test\n",
    "\n",
    "    # Test forward pass\n",
    "    Z_expected = X_test @ W_test + B_test\n",
    "    Z_output = layer.forward(X_test)\n",
    "    assert np.allclose(Z_output, Z_expected), \"Forward pass error\"\n",
    "\n",
    "    # Test backward pass\n",
    "    dL_dX_expected = dL_dZ_test @ W_test.T\n",
    "    dL_dW_expected = X_test.T @ dL_dZ_test\n",
    "    dL_dB_expected = np.sum(dL_dZ_test, axis=0)\n",
    "    \n",
    "    dL_dX_output = layer.backward(dL_dZ_test)\n",
    "    assert np.allclose(dL_dX_output, dL_dX_expected), \"Backward pass error in dL/dX\"\n",
    "    assert np.allclose(layer.dL_dW, dL_dW_expected), \"Backward pass error in dL/dW\"\n",
    "    assert np.allclose(layer.dL_dB, dL_dB_expected), \"Backward pass error in dL/dB\"\n",
    "\n",
    "    # Test parameter update\n",
    "    W_updated_expected = W_test - learning_rate * dL_dW_expected\n",
    "    B_updated_expected = B_test - learning_rate * dL_dB_expected\n",
    "\n",
    "    layer.update_parameters()\n",
    "    assert np.allclose(layer.W, W_updated_expected), \"Update error in W\"\n",
    "    assert np.allclose(layer.B, B_updated_expected), \"Update error in B\"\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "# Run tests\n",
    "test_linear_layer()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Preprocessing\n",
    "To prepare the training data for a machine learning model, a function named `preprocess_data` is first defined. This function accepts an MNIST dataset object and performs the following steps:\n",
    "\n",
    "First, the image data is normalized by scaling the pixel values of the images to the range \\([0, 1]\\). This is done by dividing each pixel value by \\(255.0\\), making the values consistent and more manageable for many machine learning algorithms. Subsequently, the image data is flattened, meaning each image, which originally has a size of \\(28 \\times 28\\) pixels, is converted into a one-dimensional vector of length \\(784\\). This conversion is important because many models, especially simple neural networks, expect inputs as 1D vectors.\n",
    "\n",
    "The target variables, i.e., the data labels, are provided in two forms: as original integer labels and as a one-hot encoded matrix. For this purpose, the function `one_hot_encode` is used, which converts each categorical label into a binary vector. The index corresponding to the label is marked with a \\(1\\), while all other positions are set to \\(0\\). For example, the label \\(3\\) is converted into the vector \\([0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\\). This one-hot encoding is particularly useful for classification tasks, as it transforms the labels into a format that can be directly compared with the outputs of classification models, especially when using Softmax or cross-entropy loss functions.\n",
    "\n",
    "After processing all image-label pairs, the lists are converted into NumPy arrays. The result is two arrays:\n",
    "- **X**: An array with the shape \\((\\text{number of samples}, 784)\\) that contains the normalized and flattened images.\n",
    "- **Y**: An array with the shape \\((\\text{number of samples}, 10)\\) that contains the one-hot encoded labels.\n",
    "\n",
    "These preprocessed data can now be used directly as input for a neural network or another machine learning model. By normalizing and flattening the images as well as one-hot encoding the labels, it is ensured that the data is in a format that can be efficiently processed by most models."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_image_data(data):\n",
    "    \"\"\"\n",
    "    Converts a list of (<PIL.Image.Image image mode=L size=28x28>, label) to \n",
    "    (numpy array of flattened images, one-hot encoded labels).\n",
    "    \n",
    "    Parameters:\n",
    "    - data: List of tuples where each tuple is (<PIL.Image.Image>, label).\n",
    "    \n",
    "    Returns:\n",
    "    - X: numpy array of shape (num_samples, 784) with normalized and flattened images.\n",
    "    - Y: numpy array of shape (num_samples, 10) with one-hot encoded labels.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for img, label in data:\n",
    "        # Convert image to numpy array and normalize pixels to the range [0, 1]\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        # Flatten the image (28x28) to a 1D array of size 784\n",
    "        img_flat = img_array.flatten()\n",
    "        X.append(img_flat)\n",
    "        \n",
    "        # Create a one-hot encoded vector for the label\n",
    "        one_hot = np.zeros(10)\n",
    "        one_hot[label] = 1\n",
    "        Y.append(one_hot)\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "# Example usage\n",
    "# Assuming `data` is a list of tuples in the format (<PIL.Image.Image image mode=L size=28x28>, label)\n",
    "# X, Y = preprocess_image_data(data)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Neural Network Overview\n",
    "\n",
    "This neural network is designed for classification tasks. It consists of an input layer, three hidden layers with ReLU activations, and an output layer with a softmax activation. The architecture is as follows:\n",
    "\n",
    "- **Input Layer**: Accepts input features of dimension `input_dim`.\n",
    "- **Hidden Layers**: Three layers with dimensions `hidden_dim`, each followed by a ReLU activation function.\n",
    "- **Output Layer**: Produces class probabilities using the softmax function.\n",
    "\n",
    "## Activation Functions\n",
    "\n",
    "### ReLU (Rectified Linear Unit)\n",
    "Used to activate hidden layers.\n",
    "Defined as:\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "- **Advantages**:\n",
    "  - Introduces non-linearity.\n",
    "  - Mitigates the vanishing gradient problem.\n",
    "  - Computationally efficient.\n",
    "\n",
    "### Softmax Function\n",
    "Used to activate the output layer.\n",
    "Defined as:\n",
    "$$\n",
    "\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}\n",
    "$$\n",
    "\n",
    "- Converts raw scores (logits) into probabilities.\n",
    "- Ensures the output probabilities sum to 1.\n",
    "\n",
    "## Cost Function\n",
    "The network uses the **Cross-Entropy Loss** for multi-class classification:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{C} y_{i,k} \\log(\\hat{y}_{i,k})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$ is the number of samples.\n",
    "- $C$ is the number of classes.\n",
    "- $y_{i,k}$ is the true label (one-hot encoded) for sample $i$ and class $k$.\n",
    "- $\\hat{y}_{i,k}$ is the predicted probability for sample $i$ and class $k$.\n",
    "\n",
    "\n",
    "\n",
    "### Why Cross-Entropy Loss?\n",
    "- It measures the discrepancy between the true distribution and the predicted probability distribution which makes it suitable for classification.\n",
    "- Penalizes very probable but incorrect predictions more than less probable ones and therefore adds interpretation based on probabilities.\n",
    "\n",
    "### Comparison with Other Loss Functions\n",
    "- **Mean Squared Error (MSE)**:\n",
    "  - *Not ideal for classification* as it treats outputs as continuous rather than categorical.\n",
    "  - *Gradient issues* can slow down learning.\n",
    "- **Hinge Loss**:\n",
    "  - Used mainly for binary classification (e.g., SVMs).\n",
    "  - Doesn't provide probability estimates.\n",
    "\n",
    "## Evaluation Metric\n",
    "The model's performance is evaluated using **Accuracy**:\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "$$\n",
    "\n",
    "### Why Accuracy?\n",
    "\n",
    "- It is easy to interpret and calculate.\n",
    "- Directly measures the proportion of correct predictions and therefore measures the most relevant metric.\n",
    "\n",
    "### Comparison with Other Metrics\n",
    "- **Precision & Recall**:\n",
    "  - Useful for imbalanced datasets.\n",
    "  - Provide insight into specific types of errors.\n",
    "- **F1 Score**:\n",
    "  - Harmonic mean of precision and recall.\n",
    "  - More informative when dealing with class imbalance.\n",
    "\n",
    "## Conclusion\n",
    "The combination of ReLU activations, softmax output, cross-entropy loss, and accuracy metric is well-suited for multi-class classification tasks. This setup:\n",
    "\n",
    "- Efficiently captures non-linear relationships.\n",
    "- Provides probabilistic outputs for interpretability.\n",
    "- Uses a loss function aligned with the goal of minimizing misclassification.\n",
    "- Employs an evaluation metric that directly reflects performance on the task.\n",
    "\n",
    "Alternative functions were considered but were less appropriate due to the nature of the problem (e.g., MSE is better for regression, and hinge loss is not probabilistic).\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.01):\n",
    "        self.layer1 = LinearLayer(input_dim, hidden_dim, learning_rate)\n",
    "        self.layer2 = LinearLayer(hidden_dim, hidden_dim, learning_rate)\n",
    "        self.layer3 = LinearLayer(hidden_dim, hidden_dim, learning_rate)\n",
    "        self.output_layer = LinearLayer(hidden_dim, output_dim, learning_rate)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Z1 = np.maximum(0, self.layer1.forward(X))  # ReLU activation\n",
    "        self.Z2 = np.maximum(0, self.layer2.forward(self.Z1))  # ReLU activation\n",
    "        self.Z3 = np.maximum(0, self.layer3.forward(self.Z2))  # ReLU activation\n",
    "        self.output = self.softmax(self.output_layer.forward(self.Z3))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dL_dY):\n",
    "        dL_dZ3 = self.output_layer.backward(dL_dY)\n",
    "        dL_dZ3[self.Z3 <= 0] = 0\n",
    "        dL_dZ2 = self.layer3.backward(dL_dZ3)\n",
    "        dL_dZ2[self.Z2 <= 0] = 0\n",
    "        dL_dZ1 = self.layer2.backward(dL_dZ2)\n",
    "        dL_dZ1[self.Z1 <= 0] = 0\n",
    "        self.layer1.backward(dL_dZ1)\n",
    "\n",
    "    def update_parameters(self):\n",
    "        self.layer1.update_parameters()\n",
    "        self.layer2.update_parameters()\n",
    "        self.layer3.update_parameters()\n",
    "        self.output_layer.update_parameters()\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "        return expZ / expZ.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, Y_pred, Y_true):\n",
    "        n_samples = Y_true.shape[0]\n",
    "        log_likelihood = -np.log(Y_pred[range(n_samples), Y_true.argmax(axis=1)])\n",
    "        return np.sum(log_likelihood) / n_samples\n",
    "\n",
    "    def compute_accuracy(self, Y_pred, Y_true):\n",
    "        predictions = np.argmax(Y_pred, axis=1)\n",
    "        labels = np.argmax(Y_true, axis=1)\n",
    "        return np.mean(predictions == labels)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training the Neural Network\n",
    "In this section the neural network will be trained using the MNIST dataset. The training process involves multiple steps, including forward and backward passes, parameter updates, and monitoring key metrics like loss and accuracy. The training loop will iterate over the dataset for a specified number of epochs, updating the network's parameters to minimize the loss and improve accuracy. Within the epochs only parts of the dataset are used to train the network, so-called batches.\n",
    "\n",
    "#### Why Batch Training?\n",
    "- **Faster Training:** Smaller batches allow the neural network to process data more efficiently, reducing training time.\n",
    "- **Better Accuracy:** Diverse examples in batches help the network learn more effectively, improving task performance.\n",
    "- **Enhanced Generalization:** Learning from varied data improves the network's ability to handle unseen examples.\n",
    "\n",
    "For this implementation we chose to use mini-batch gradient descent. Where the data is divided into smaller batches and gradient descent is applied to each one of them."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training Loop\n",
    "def train_network(network, X_train, Y_train, X_test, Y_test, epochs=5, batch_size=32):\n",
    "    train_history = defaultdict(list)\n",
    "    test_history = defaultdict(list)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle the data\n",
    "        indices = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        X_train = X_train[indices]\n",
    "        Y_train = Y_train[indices]\n",
    "\n",
    "        # Mini-batch training\n",
    "        for start in range(0, X_train.shape[0], batch_size):\n",
    "            end = start + batch_size\n",
    "            X_batch = X_train[start:end]\n",
    "            Y_batch = Y_train[start:end]\n",
    "\n",
    "            # Forward pass\n",
    "            Y_pred = network.forward(X_batch)\n",
    "\n",
    "            # Backward pass and parameter update\n",
    "            dL_dY = Y_pred - Y_batch  # Derivative of loss with respect to predictions\n",
    "            network.backward(dL_dY)\n",
    "            network.update_parameters()\n",
    "\n",
    "        Y_train_pred = network.forward(X_train)\n",
    "        train_loss = network.cross_entropy_loss(Y_train_pred, Y_train)\n",
    "        train_accuracy = network.compute_accuracy(Y_train_pred, Y_train)\n",
    "        train_history['loss'].append(train_loss)\n",
    "        train_history['accuracy'].append(train_accuracy)\n",
    "\n",
    "        Y_test_pred = network.forward(X_test)\n",
    "        test_loss = network.cross_entropy_loss(Y_test_pred, Y_test)\n",
    "        test_accuracy = network.compute_accuracy(Y_test_pred, Y_test)\n",
    "        test_history['loss'].append(test_loss)\n",
    "        test_history['accuracy'].append(test_accuracy)\n",
    "\n",
    "    return train_history, test_history"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluating the Neural Network\n",
    "\n",
    "In this part of the project, we evaluate the neural network that was created and trained to classify images from the MNIST dataset. Specifically, we aimed to assess the impact of various hyperparameters, such as learning rate and hidden layer size, on the performance of the model.\n",
    "\n",
    "We trained the network with multiple combinations of learning rates (0.001, 0.01, 0.1) and hidden layer sizes (4, 8, 16). The training process involved monitoring key metrics like training and testing accuracy, as well as loss values over 10 epochs for each hyperparameter combination. These metrics allow us to gauge how well the model is learning and to identify trends that can help optimize its performance.\n",
    "\n",
    "The results were plotted to visualize the effects of different hyperparameters on both training and testing performance. The plots include:\n",
    "\n",
    "- **Training Accuracy**: How accurately the network predicted the training images for each hyperparameter combination.\n",
    "- **Training Loss**: The reduction in error during training, indicating the effectiveness of gradient descent.\n",
    "- **Test Accuracy**: The performance of the network on unseen test images, which reflects the generalization ability.\n",
    "- **Test Loss**: The error rate on the test set, helping us determine if the network is overfitting or underfitting.\n",
    "\n",
    "These visualizations provide an overview of which hyperparameter combinations led to optimal performance, helping us make informed decisions about model tuning. Overall, this evaluation approach helps ensure the robustness of the neural network and its ability to accurately classify new data.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Preprocessing the MNIST dataset for the Training and Test data\n",
    "X_train, Y_train = preprocess_image_data(mnist_trainset)\n",
    "X_test, Y_test = preprocess_image_data(mnist_testset)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "hidden_layer_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionary to store results for different hyperparameters\n",
    "results = {}\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for hidden_size in hidden_layer_sizes:\n",
    "        print(f\"Training with learning rate {lr} and hidden layer size {hidden_size}\")\n",
    "        network = NeuralNetwork(input_dim=784, hidden_dim=hidden_size, output_dim=10, learning_rate=lr)\n",
    "        train_history, test_history = train_network(network, X_train, Y_train, X_test, Y_test, epochs=10)\n",
    "        results[(lr, hidden_size)] = {\n",
    "            'train_history': train_history,\n",
    "            'test_history': test_history\n",
    "        }\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function which plots the results from the hyperparameter tuning\n",
    "def plot_results(results):\n",
    "    learning_rates = sorted({lr for (lr, _) in results.keys()})\n",
    "    hidden_layer_sizes = sorted({size for (_, size) in results.keys()})\n",
    "\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    labels = []\n",
    "\n",
    "# Extracting the results for each hyperparameter combination into separate list so plotting can be done\n",
    "    for lr in learning_rates:\n",
    "        for size in hidden_layer_sizes:\n",
    "            key = (lr, size)\n",
    "            if key in results:\n",
    "                train_accuracies.append(results[key]['train_history']['accuracy'][-1])\n",
    "                test_accuracies.append(results[key]['test_history']['accuracy'][-1])\n",
    "                train_losses.append(results[key]['train_history']['loss'][-1])\n",
    "                test_losses.append(results[key]['test_history']['loss'][-1])\n",
    "                labels.append(f\"LR: {lr}, HSize: {size}\")\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "\n",
    "    # Plotting Training Accuracy\n",
    "    axs[0, 0].plot(x, train_accuracies, marker='o', label='Train Accuracy', color='blue')\n",
    "    axs[0, 0].set_xticks(x)\n",
    "    axs[0, 0].set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    axs[0, 0].set_ylabel('Accuracy')\n",
    "    axs[0, 0].set_title('Training Accuracy for Different Hyperparameters')\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    # Plotting Training Loss\n",
    "    axs[0, 1].plot(x, train_losses, marker='o', label='Train Loss', color='blue')\n",
    "    axs[0, 1].set_xticks(x)\n",
    "    axs[0, 1].set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    axs[0, 1].set_ylabel('Loss')\n",
    "    axs[0, 1].set_title('Training Loss for Different Hyperparameters')\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    # Plotting Test Accuracy\n",
    "    axs[1, 0].plot(x, test_accuracies, marker='o', label='Test Accuracy', color='orange')\n",
    "    axs[1, 0].set_xticks(x)\n",
    "    axs[1, 0].set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    axs[1, 0].set_ylabel('Accuracy')\n",
    "    axs[1, 0].set_title('Test Accuracy for Different Hyperparameters')\n",
    "    axs[1, 0].legend()\n",
    "\n",
    "    # Plotting Test Loss\n",
    "    axs[1, 1].plot(x, test_losses, marker='o', label='Test Loss', color='orange')\n",
    "    axs[1, 1].set_xticks(x)\n",
    "    axs[1, 1].set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    axs[1, 1].set_ylabel('Loss')\n",
    "    axs[1, 1].set_title('Test Loss for Different Hyperparameters')\n",
    "    axs[1, 1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Plotting the results from the hyperparameter tuning    \n",
    "plot_results(results)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Results Analysis\n",
    "\n",
    "The evaluation of different hyperparameters shows their significant impact on network performance.\n",
    "\n",
    "- **Accuracy & Loss**: Both training and test accuracies are high for some combinations, but steep drops occur with unsuitable parameters. Loss patterns confirm that higher learning rates and small hidden layers can destabilize training.\n",
    "\n",
    "- **Hyperparameter Impact**: A moderate learning rate with balanced hidden layer size tends to provide the best results, balancing learning speed, accuracy, and stability.\n",
    "\n",
    "The key takeaway is the need for balanced hyperparameter tuning to achieve both effective learning and generalization.\n",
    "In this case a Learning Rate of 0.001 and Hidden Layer Size of 16 seems to be the highest achieving combination.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Summary\n",
    "This project focused on applying gradient descent to train neural networks for image classification. Using the MNIST dataset of handwritten digits, we first analyzed the data and found a uniform distribution of the dataset, which provided us with a solid foundation for training.\n",
    "\n",
    "First we dove into the theoretical aspects of neural networks, covering essential components such as the structure of neural networks, activation functions (ReLU and Softmax), cost functions (Cross-Entropy Loss), and evaluation metrics (Accuracy). These concepts were crucial for building and understanding our neural network model.\n",
    "\n",
    "The neural network implemented consists of:\n",
    "- **Input Layer**: Accepting flattened image data.\n",
    "- **Three Hidden Layers**: Each with ReLU activation to introduce non-linearity.\n",
    "- **Output Layer**: Utilizing Softmax activation to produce class probabilities.\n",
    "\n",
    "Data preprocessing involved normalizing pixel values, flattening the 28x28 images into 1D vectors, and one-hot encoding the labels for efficient processing.\n",
    "\n",
    "We trained the network using mini-batch gradient descent, which improves computational efficiency and generalization by updating model parameters in small batches.\n",
    "\n",
    "During training, we compared different hyperparameter combinations, including learning rates and hidden layer sizes, to evaluate their impact on model performance. The results were visualized to identify optimal hyperparameters.\n",
    "\n",
    "By experimenting with various combinations of learning rates (0.001, 0.01, 0.1) and hidden layer sizes (4, 8, 16), we found that a learning rate of **0.001** and a hidden layer size of **16** yielded the best performance. This combination provided high accuracy and stability, highlighting the significant impact of hyperparameter tuning on model performance.\n",
    "\n",
    "In conclusion, the project demonstrated the practical application of gradient descent in training neural networks for image classification."
   ]
  }
 ]
}
